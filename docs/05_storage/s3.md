# Pipeline Storage

The OC Fetcher framework provides comprehensive AWS S3 storage integration with metadata management, environment-specific configurations, and enterprise-grade features. Pipeline storage is designed for production deployments with automatic bucket management, metadata preservation, and performance optimization.

## Pipeline Storage Features

### **Automatic Bucket Management**
- Environment-specific bucket naming
- Automatic bucket creation and configuration
- Bucket policy management and security settings

### **Metadata Management**
- Comprehensive metadata storage alongside data
- Custom metadata support for application-specific information
- Metadata indexing and retrieval capabilities

### **Performance Optimization**
- Streaming uploads for large files
- Parallel uploads for improved performance
- Intelligent chunking and multipart uploads

### **Security and Compliance**
- AWS IAM integration for access control
- Encryption at rest and in transit
- Audit logging and compliance reporting

## Pipeline Storage Configuration

### **Basic Pipeline Configuration**
```python
from data_fetcher_core.storage import PipelineStorage

# Create pipeline storage
storage = PipelineStorage(
    bucket="my-fetcher-bucket",
    prefix="data/",
    region="us-east-1"
)
```

### **Environment-Specific Configuration**
```python
from data_fetcher_core.storage import PipelineStorage
from data_fetcher_core.global_storage import configure_application_storage

# Configure application storage
configure_application_storage()

# Environment-specific bucket naming
bucket_name = f"oc-fetcher-{config.environment}"
storage = PipelineStorage(
    bucket=bucket_name,
    prefix="data/",
    region=config.aws.region
)
```

### **Advanced S3 Configuration**
```python
from data_fetcher_core.storage import PipelineStorage

# Advanced configuration with custom settings
storage = PipelineStorage(
    bucket="my-bucket",
    prefix="fetcher/",
    region="us-east-1",
    encryption="AES256",
    lifecycle_policy="archive-after-30-days",
    versioning=True
)
```

## Pipeline Storage Usage

### **Basic Storage Operations**
```python
from data_fetcher_core.storage import PipelineStorage

# Create pipeline storage
storage = PipelineStorage("my-bucket", "data/")

# Open bundle for writing
async with storage.open_bundle("bundle-1", {"source": "example.com"}) as bundle:
    # Write resource to S3
    await bundle.write_resource(
        url="https://example.com/data",
        content_type="application/json",
        status_code=200,
        stream=data_stream
    )
```

### **Pipeline Storage with Decorators**
```python
from data_fetcher_core.storage import PipelineStorage, create_storage_stack

# Create pipeline storage with decorators
base_storage = PipelineStorage("my-bucket", "data/")
storage = create_storage_stack(
    base_storage=base_storage,

    bundle_resources=True,
    unzip_resources=True
)
```

### **Custom S3 Configuration**
```python
from data_fetcher_core.storage import PipelineStorage, BundleResourcesDecorator

# Custom S3 configuration with resource bundling
base_storage = PipelineStorage(
    bucket="my-bundle-bucket",
    prefix="bundles/",
    region="us-east-1"
)
storage = BundleResourcesDecorator(base_storage)
```

## AWS Integration

### **AWS Credentials**
The S3 storage component automatically uses AWS credentials from the following sources:

1. **Environment Variables**:
```bash
export AWS_ACCESS_KEY_ID=your-access-key
export AWS_SECRET_ACCESS_KEY=your-secret-key
export AWS_DEFAULT_REGION=us-east-1
```

2. **AWS Credentials File**:
```ini
# ~/.aws/credentials
[default]
aws_access_key_id = your-access-key
aws_secret_access_key = your-secret-key
```

3. **IAM Roles** (for EC2 instances):
```python
# Automatic credential detection for EC2 instances
storage = PipelineStorage("my-bucket", "data/")
```

### **AWS Configuration**
```python
from data_fetcher_core.global_storage import configure_application_storage

# Configure application storage
configure_application_storage()

# AWS configuration
config.aws.region = "us-east-1"
config.aws.profile = "production"
config.aws.role_arn = "arn:aws:iam::123456789012:role/FetcherRole"
```

## Pipeline Storage Features

### **Bucket Management**
```python
from data_fetcher_core.storage import PipelineStorage

# Automatic bucket creation
storage = PipelineStorage(
    bucket="my-new-bucket",
    create_bucket=True,
    bucket_policy="public-read"
)
```

### **Lifecycle Policies**
```python
from data_fetcher_core.storage import PipelineStorage

# Configure lifecycle policies
storage = PipelineStorage(
    bucket="my-bucket",
    lifecycle_policy={
        "transitions": [
            {
                "days": 30,
                "storage_class": "STANDARD_IA"
            },
            {
                "days": 90,
                "storage_class": "GLACIER"
            }
        ],
        "expiration": {
            "days": 365
        }
    }
)
```

### **Encryption and Security**
```python
from data_fetcher_core.storage import PipelineStorage

# Configure encryption
storage = PipelineStorage(
    bucket="my-bucket",
    encryption="AES256",  # or "aws:kms" for KMS encryption
    kms_key_id="arn:aws:kms:us-east-1:123456789012:key/my-key"
)
```

## Performance Optimization

### **Multipart Uploads**
```python
from data_fetcher_core.storage import PipelineStorage

# Configure multipart uploads
storage = PipelineStorage(
    bucket="my-bucket",
    multipart_threshold=100 * 1024 * 1024,  # 100MB
    multipart_chunksize=10 * 1024 * 1024    # 10MB chunks
)
```

### **Parallel Uploads**
```python
from data_fetcher_core.storage import PipelineStorage

# Configure parallel uploads
storage = PipelineStorage(
    bucket="my-bucket",
    max_concurrent_uploads=10,
    upload_timeout=300  # 5 minutes
)
```

## Monitoring and Metrics

### **S3 Metrics**
The S3 storage component provides built-in metrics:

```python
from data_fetcher_core.storage import PipelineStorage
import structlog

logger = structlog.get_logger()

# S3 storage with metrics
storage = PipelineStorage("my-bucket", "data/")

# Metrics are automatically logged
logger.info(
    "S3 upload completed",
    bucket="my-bucket",
    key="data/bundle-1",
    size=1024,
    duration=1.5
)
```

### **CloudWatch Integration**
```python
from data_fetcher_core.storage import PipelineStorage

# CloudWatch metrics
storage = PipelineStorage(
    bucket="my-bucket",
    cloudwatch_metrics=True,
    metrics_namespace="OCFetcher"
)
```

## Error Handling

### **Retry Logic**
```python
from data_fetcher_core.storage import PipelineStorage

# Configure retry logic
storage = PipelineStorage(
    bucket="my-bucket",
    max_retries=3,
    retry_delay=1,  # seconds
    exponential_backoff=True
)
```

### **Error Recovery**
```python
from data_fetcher_core.storage import PipelineStorage

try:
    storage = PipelineStorage("my-bucket", "data/")
    # Storage operations
except Exception as e:
    logger.exception("S3 storage error", error=str(e))
    # Handle error and retry
```

## Key Features

- **Automatic Bucket Management**: Environment-specific bucket naming and creation
- **Metadata Preservation**: Comprehensive metadata storage alongside data
- **Performance Optimization**: Streaming uploads and parallel processing
- **Security**: AWS IAM integration and encryption support
- **Monitoring**: Built-in metrics and CloudWatch integration
- **Error Handling**: Robust retry logic and error recovery
- **Compliance**: Audit logging and compliance reporting
- **Scalability**: Support for large datasets and high-throughput operations
